{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58aa4f3-144c-4767-9872-d56685a33c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from tabulate import tabulate \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001023bc-7ddc-4ffe-88dd-0ea42b22a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\", \n",
    "           \"I enjoy hiking and camping in the mountains\", \n",
    "           \"I like to read books and watch movies\", \n",
    "           \"I prefer playing video games over sports\", \n",
    "           \"I love listening to music and going to concerts\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d043fa-0cdc-4def-8473-f4d5ce898990",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() \n",
    "X = vectorizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256f6d8e-1e46-4386-837b-5c15ceb1d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " to\n",
      " and\n",
      " read\n",
      " watch\n",
      " movies\n",
      " like\n",
      " books\n",
      " concerts\n",
      " going\n",
      " music\n",
      "\n",
      "Cluster 1:\n",
      " playing\n",
      " the\n",
      " weekends\n",
      " on\n",
      " football\n",
      " video\n",
      " sports\n",
      " prefer\n",
      " over\n",
      " games\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 2  # Define the number of clusters \n",
    "km = KMeans(n_clusters=k) \n",
    "km.fit(X) \n",
    " \n",
    "# Predict the clusters for each document \n",
    "y_pred = km.predict(X) \n",
    " \n",
    "# Display the document and its predicted cluster in a table \n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]] \n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)]) \n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "# Print top terms per cluster \n",
    "print(\"\\nTop terms per cluster:\") \n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "terms = vectorizer.get_feature_names_out() \n",
    "for i in range(k): \n",
    "    print(\"Cluster %d:\" % i) \n",
    "    for ind in order_centroids[i, :10]: \n",
    "        print(' %s' % terms[ind]) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a4078d-c69e-4b3d-8a92-eab97843526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity \n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45812e3e-b7b7-4d2a-8e7f-ad2ed4b60575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "from gensim.models import Word2Vec \n",
    "from tabulate import tabulate \n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "302c455b-f2e6-4b2a-8c9e-ad1cc0c0498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\", \n",
    "           \"I enjoy hiking and camping in the mountains\", \n",
    "           \"I like to read books and watch movies\", \n",
    "           \"I prefer playing video games over sports\", \n",
    "           \"I love listening to music and going to concerts\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a1f516a-3e42-4710-8a58-7c808f964f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = [doc.split() for doc in dataset] \n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, \n",
    "window=5, min_count=1, workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68c7ebe9-e799-43a0-8b83-e18a5b63b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in \n",
    "word2vec_model.wv], axis=0) for doc in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3048229-bd3f-4491-a246-6a40ea7f8c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k = 2  # Define the number of clusters \n",
    "km = KMeans(n_clusters=k) \n",
    "km.fit(X) \n",
    " \n",
    "# Predict the clusters for each document \n",
    "y_pred = km.predict(X) \n",
    " \n",
    "# Tabulate the document and predicted cluster \n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]] \n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)]) \n",
    "print(tabulate(table_data, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e237ae1b-6588-420f-9e7e-7352511340ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity \n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140c4f9a-2f96-4d25-bbd2-05e4d8728fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99f2ca8-2217-4bc7-b207-293ccc212b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\victus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f5933e-338b-4f13-ad3a-ba42ece116f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"customer_complaints_1.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd07ab50-0be8-4414-8ae3-c4d7b5d9d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ae5ad1-1e94-43cc-8f0c-886e832f8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessed_text\"] = df[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c797e18-5b05-46d0-b155-ec953758d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"preprocessed_text\"])\n",
    "\n",
    "k = 5  # Number of clusters\n",
    "kmeans_tfidf = KMeans(n_clusters=k, random_state=0)\n",
    "y_pred_tfidf = kmeans_tfidf.fit_predict(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3bb524f-5170-47d0-9e0f-b43cfa38740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity (TF-IDF with preprocessing): 1.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_purity(y_pred):\n",
    "    counter = Counter(y_pred)\n",
    "    purity = sum(counter.values()) / len(y_pred)\n",
    "    return purity\n",
    "\n",
    "purity_tfidf = calculate_purity(y_pred_tfidf)\n",
    "print(\"Purity (TF-IDF with preprocessing):\", purity_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "311489e2-2571-4b13-b0ec-243a4d3925a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity (Word2Vec with preprocessing): 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = [text.split() for text in df[\"preprocessed_text\"]]\n",
    "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(doc):\n",
    "    vectors = [w2v_model.wv[word] for word in doc if word in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "X_w2v = np.array([document_vector(doc) for doc in tokenized_docs])\n",
    "\n",
    "kmeans_w2v = KMeans(n_clusters=k, random_state=0)\n",
    "y_pred_w2v = kmeans_w2v.fit_predict(X_w2v)\n",
    "\n",
    "purity_w2v = calculate_purity(y_pred_w2v)\n",
    "print(\"Purity (Word2Vec with preprocessing):\", purity_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc212ecc-fff8-4496-9d21-7a2978391132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  TFIDF_Cluster  \\\n",
      "0  I used to love Comcast. Until all these consta...              3   \n",
      "1  I'm so over Comcast! The worst internet provid...              3   \n",
      "2  If I could give them a negative star or no sta...              0   \n",
      "3  I've had the worst experiences so far since in...              2   \n",
      "4  Check your contract when you sign up for Comca...              0   \n",
      "\n",
      "   Word2Vec_Cluster  \n",
      "0                 1  \n",
      "1                 1  \n",
      "2                 1  \n",
      "3                 1  \n",
      "4                 0  \n"
     ]
    }
   ],
   "source": [
    "df['TFIDF_Cluster'] = y_pred_tfidf\n",
    "df['Word2Vec_Cluster'] = y_pred_w2v\n",
    "print(df[['text', 'TFIDF_Cluster', 'Word2Vec_Cluster']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5a2f5-a4aa-403b-85a4-8c288b2b9612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
